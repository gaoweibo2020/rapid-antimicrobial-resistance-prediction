{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "\n",
    "#SNIP algrithm\n",
    "def snip(y, iterations=20, decreasing=True):\n",
    "    n = len(y)\n",
    "    d = int(decreasing)\n",
    "\n",
    "    xo = np.empty(n, dtype=np.float64)\n",
    "    xy = np.array(y, dtype=np.float64)\n",
    "\n",
    "    k = int(iterations)\n",
    "\n",
    "    # code duplication to use faster ++i/--i instead of i+=step\n",
    "    if d:\n",
    "        for i in range(k, 0, -1):\n",
    "            for j in range(i, n - i):\n",
    "                a = xy[j]\n",
    "                b = (xy[j - i] + xy[j + i]) / 2\n",
    "                if b < a:\n",
    "                    a = b\n",
    "                xo[j] = a\n",
    "\n",
    "            for j in range(i, n - i):\n",
    "                xy[j] = xo[j]\n",
    "    else:\n",
    "        for i in range(1, k + 1):\n",
    "            for j in range(i, n - i):\n",
    "                a = xy[j]\n",
    "                b = (xy[j - i] + xy[j + i]) / 2\n",
    "                if b < a:\n",
    "                    a = b\n",
    "                xo[j] = a\n",
    "\n",
    "            for j in range(i, n - i):\n",
    "                xy[j] = xo[j]\n",
    "\n",
    "    xo = xy.copy()\n",
    "\n",
    "    return xo\n",
    "\n",
    "\n",
    "# intensity calibration\n",
    "def toc(y_data):\n",
    "    return y_data / y_data.sum()\n",
    "\n",
    "\n",
    "def interpolate(data, boundary, mask, kind):\n",
    "    if kind != None:\n",
    "        f = interp1d(data[0],\n",
    "                     data[1],\n",
    "                     kind=kind,\n",
    "                     bounds_error=False,\n",
    "                     fill_value=0,\n",
    "                     assume_sorted=False)\n",
    "    new_data = []\n",
    "    if kind != None:\n",
    "        for i in range(len(boundary)):\n",
    "            new_data.append(f(boundary[i]))\n",
    "    else:\n",
    "        for i in range(len(boundary)):\n",
    "            if i + 1 == len(boundary):\n",
    "                break\n",
    "            if ((data[0] < boundary[i + 1]) &\n",
    "                (data[0] > boundary[i])).astype('int').sum() != 0:\n",
    "                new_data.append(data[1][(data[0] < boundary[i + 1])\n",
    "                                        & (data[0] > boundary[i])].max())\n",
    "            else:\n",
    "                new_data.append(0)\n",
    "    new_data = np.array(new_data)\n",
    "\n",
    "    new_data[new_data < 0] = 0\n",
    "    if mask is not None:\n",
    "        new_data = new_data[mask]\n",
    "    return new_data\n",
    "\n",
    "\n",
    "def SampleGenerator(data, boundary, housekeeping, mask=None, kind=None, shuffle=True):\n",
    "    if shuffle:\n",
    "        data = data.sample(frac=1).reset_index(drop=True)\n",
    "    ids = data['ID']\n",
    "    x = []\n",
    "    for id in ids:\n",
    "        x.append(load_mass('./data/' + id + '.txt', boundary, mask, kind, housekeeping))\n",
    "    x = np.array(x)\n",
    "\n",
    "    label = data['Class']\n",
    "    y = np.zeros_like(label)\n",
    "    y[label == 'S'] = 1\n",
    "    y = y.astype('float')\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def load_mass(path, boundary, mask, kind,housekeeping):\n",
    "    data = [[], []]\n",
    "\n",
    "    with open(path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        # remove comments\n",
    "        lines = lines[8:]\n",
    "        for line in lines:\n",
    "            data[0].append(float(line.split(' ')[0]))\n",
    "            data[1].append(float(line.split(' ')[1]))\n",
    "    data = np.array(data)\n",
    "\n",
    "    if housekeeping is not None:\n",
    "        min_diff = 1e5\n",
    "        for item in data[0]:\n",
    "            diff = housekeeping - item\n",
    "            if np.abs(diff) < np.abs(min_diff):\n",
    "                min_diff = diff\n",
    "        data[0] = data[0] + min_diff\n",
    "\n",
    "    # variance stabilising\n",
    "    data[1] = np.sqrt(data[1])\n",
    "    # smoothing\n",
    "    data[1] = savgol_filter(data[1], window_length=21,\n",
    "                            polyorder=3)\n",
    "    data[1][data[1] < 0] = 0\n",
    "    # baseline removal\n",
    "    data[1] = data[1] - snip(data[1].copy())\n",
    "    # intensity calibration\n",
    "    data[1] = toc(data[1])\n",
    "\n",
    "    #bin\n",
    "    new_data = interpolate(data, boundary, mask, kind=kind)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "data = pd.read_csv('../data/data.csv')\n",
    "test_ptient = data.Patient.sample(80)\n",
    "test_ptient_data = data[data.Patient.isin(test_ptient)]\n",
    "train_data = data.drop(test_ptient_data.index).reset_index(drop=True)\n",
    "test_data = test_ptient_data.reset_index(drop=True)\n",
    "boundary = np.arange(0, 1801) * 5 + 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mass_point(path):\n",
    "    s_data = [[], []]\n",
    "    with open(path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        lines = lines[8:]\n",
    "        for line in lines:\n",
    "            s_data[0].append(float(line.split(' ')[0]))\n",
    "            s_data[1].append(float(line.split(' ')[1]))\n",
    "    return np.array(s_data)\n",
    "\n",
    "\n",
    "housekeeping = 4428\n",
    "m_z_freq = [0 for i in range(30000)]\n",
    "for id in data.ID:\n",
    "    x = load_mass_point('../data/' + id + '.txt')\n",
    "    int_x = np.int32(x[0])\n",
    "    min_diff=1e5\n",
    "    for item in int_x:\n",
    "        m_z_freq[item] += 1\n",
    "        diff = 4428 - item\n",
    "        if np.abs(diff) < np.abs(min_diff):\n",
    "            min_diff = diff\n",
    "    if np.abs(min_diff) > 5:\n",
    "        print(min_diff)\n",
    "        print(id)\n",
    "\n",
    "m_z_freq = np.array(m_z_freq)\n",
    "m_z_freq[4428],len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(LinearSVC(penalty='l1',\n",
    "                              loss='squared_hinge',\n",
    "                              dual=False,\n",
    "                              fit_intercept=True),\n",
    "                    param_grid={\n",
    "                        \"C\": [0.1, 0.5, 1, 5, 10],\n",
    "                        \"max_iter\": [1000, 5000, 10000, 50000, 100000]\n",
    "                    },\n",
    "                    cv=5)\n",
    "x_train, y_train = SampleGenerator(train_data,boundary,housekeeping)\n",
    "grid.fit(x_train, y_train)\n",
    "print(\"The best parameters are %s with a score of %0.2f\" %\n",
    "      (grid.best_params_, grid.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = LinearSVC(penalty='l1',\n",
    "                       loss='squared_hinge',\n",
    "                       dual=False,\n",
    "                       fit_intercept=True,\n",
    "                       **grid.best_params_)\n",
    "best_model.fit(x_train, y_train)\n",
    "\n",
    "x_test, y_test = SampleGenerator(test_data, boundary, housekeeping)\n",
    "expected = y_test\n",
    "predicted = best_model.predict(x_test)\n",
    "\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, predicted)\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.abs(best_model.coef_[0]) > 0\n",
    "print(f'useful feature dim: {mask.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mask = [mask]\n",
    "x_train, y_train = SampleGenerator(train_data,boundary,housekeeping)\n",
    "x_test, y_test = SampleGenerator(test_data, boundary, housekeeping)\n",
    "while True:\n",
    "    grid = GridSearchCV(LinearSVC(penalty='l1',\n",
    "                                  loss='squared_hinge',\n",
    "                                  dual=False,\n",
    "                                  fit_intercept=True),\n",
    "                        param_grid={\n",
    "                            \"C\": [0.1, 0.5, 1, 5, 10],\n",
    "                            \"max_iter\": [1000, 5000, 10000, 50000, 100000]\n",
    "                        },\n",
    "                        cv=5)\n",
    "    x_train = x_train[:, feature_mask[-1]]\n",
    "    grid.fit(x_train, y_train)\n",
    "    print(\"The best parameters are %s with a score of %0.2f\" %\n",
    "          (grid.best_params_, grid.best_score_))\n",
    "\n",
    "    model = LinearSVC(penalty='l1',\n",
    "                      loss='squared_hinge',\n",
    "                      dual=False,\n",
    "                      fit_intercept=True,\n",
    "                      **grid.best_params_)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    x_test = x_test[:, feature_mask[-1]]\n",
    "    expected = y_test\n",
    "    predicted = model.predict(x_test)\n",
    "\n",
    "    print(metrics.classification_report(expected,\n",
    "                                        predicted))\n",
    "    print(metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "    auc = metrics.roc_auc_score(y_test, predicted)\n",
    "    accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    mask = np.abs(model.coef_[0]) > 0\n",
    "    print(f'useful feature dim: {mask.sum()}')\n",
    "\n",
    "    final_model = model\n",
    "    final_mask = np.zeros_like(feature_mask[0])\n",
    "    mask_str = 'feature bin: '\n",
    "    for i in range(len(mask)):\n",
    "        if mask[i] > 0:\n",
    "            index = i\n",
    "            for j in range(len(feature_mask) - 1, -1, -1):\n",
    "                it = 0\n",
    "                for k in range(len(feature_mask[j])):\n",
    "                    if feature_mask[j][k] > 0:\n",
    "                        if it == index:\n",
    "                            index = k\n",
    "                            break\n",
    "                        it += 1\n",
    "            final_mask[index] = 1\n",
    "            mask_str += f'{2000+5*index}~{2000+5*(index+1)}  '\n",
    "    print(mask_str)\n",
    "    feature_mask.append(mask)\n",
    "    if mask.sum() == len(mask):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('..//datadata.csv')\n",
    "repeat_mask = np.zeros([1800])\n",
    "for repeat in range(100):\n",
    "    test_ptient = data.Patient.sample(80)\n",
    "    test_ptient_data = data[data.Patient.isin(test_ptient)]\n",
    "    train_data = data.drop(test_ptient_data.index).reset_index(drop=True)\n",
    "    test_data = test_ptient_data.reset_index(drop=True)\n",
    "    #调参\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    grid = GridSearchCV(LinearSVC(penalty='l1',\n",
    "                                  loss='squared_hinge',\n",
    "                                  dual=False,\n",
    "                                  fit_intercept=True),\n",
    "                        param_grid={\n",
    "                            \"C\": [0.1, 0.5, 1, 5, 10],\n",
    "                            \"max_iter\": [1000, 5000, 10000, 50000, 100000]\n",
    "                        },\n",
    "                        cv=5)\n",
    "    x_train, y_train = SampleGenerator(train_data, boundary, housekeeping)\n",
    "    grid.fit(x_train, y_train)\n",
    "    print(\"The best parameters are %s with a score of %0.2f\" %\n",
    "          (grid.best_params_, grid.best_score_))\n",
    "    best_model = LinearSVC(penalty='l1',\n",
    "                           loss='squared_hinge',\n",
    "                           C=grid.best_params_['C'],\n",
    "                           dual=False,\n",
    "                           max_iter=grid.best_params_['max_iter'],\n",
    "                           fit_intercept=True)\n",
    "    best_model.fit(x_train, y_train)\n",
    "    x_test, y_test = SampleGenerator(test_data, boundary, housekeeping)\n",
    "    expected = y_test\n",
    "    predicted = best_model.predict(x_test)\n",
    "    print(metrics.classification_report(expected,\n",
    "                                        predicted))\n",
    "    print(metrics.confusion_matrix(expected, predicted))\n",
    "    accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    mask = np.abs(best_model.coef_[0]) > 0\n",
    "    print(f'useful feature dim: {mask.sum()}')\n",
    "\n",
    "    feature_mask = [mask]\n",
    "    x_train, y_train = SampleGenerator(train_data,boundary,housekeeping)\n",
    "    x_test, y_test = SampleGenerator(test_data, boundary, housekeeping)\n",
    "    while True:\n",
    "        grid = GridSearchCV(LinearSVC(penalty='l1',\n",
    "                                      loss='squared_hinge',\n",
    "                                      dual=False,\n",
    "                                      fit_intercept=True),\n",
    "                            param_grid={\n",
    "                                \"C\": [0.1, 0.5, 1, 5, 10],\n",
    "                                \"max_iter\": [1000, 5000, 10000, 50000, 100000]\n",
    "                            },\n",
    "                            cv=5)\n",
    "        x_train = x_train[:, feature_mask[-1]]\n",
    "        grid.fit(x_train, y_train)\n",
    "        print(\"The best parameters are %s with a score of %0.2f\" %\n",
    "              (grid.best_params_, grid.best_score_))\n",
    "\n",
    "        model = LinearSVC(penalty='l1',\n",
    "                          loss='squared_hinge',\n",
    "                          C=grid.best_params_['C'],\n",
    "                          dual=False,\n",
    "                          max_iter=grid.best_params_['max_iter'],\n",
    "                          fit_intercept=True)\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        x_test = x_test[:, feature_mask[-1]]\n",
    "        expected = y_test\n",
    "        predicted = model.predict(x_test)\n",
    "\n",
    "        # 输出结果\n",
    "        print(metrics.classification_report(expected,\n",
    "                                            predicted))\n",
    "        print(metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "        auc = metrics.roc_auc_score(y_test, predicted)\n",
    "        accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "        print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "        mask = np.abs(model.coef_[0]) > 0\n",
    "        print(f'useful feature dim: {mask.sum()}')\n",
    "\n",
    "        final_model = model\n",
    "        final_mask = np.zeros_like(feature_mask[0])\n",
    "        mask_str = 'feature bin: '\n",
    "        for i in range(len(mask)):\n",
    "            if mask[i] > 0:\n",
    "                index = i\n",
    "                for j in range(len(feature_mask) - 1, -1, -1):\n",
    "                    it = 0\n",
    "                    for k in range(len(feature_mask[j])):\n",
    "                        if feature_mask[j][k] > 0:\n",
    "                            if it == index:\n",
    "                                index = k\n",
    "                                break\n",
    "                            it += 1\n",
    "                final_mask[index] = 1\n",
    "                mask_str += f'{2000+5*index}~{2000+5*(index+1)}  '\n",
    "        print(mask_str)\n",
    "        feature_mask.append(mask)\n",
    "        if mask.sum() == len(mask):\n",
    "            break\n",
    "    repeat_mask += final_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((repeat_mask >= 100).sum())\n",
    "data = pd.read_csv('../data/data.csv')\n",
    "test_ptient = data.Patient.sample(80)\n",
    "test_ptient_data = data[data.Patient.isin(test_ptient)]\n",
    "train_data = data.drop(test_ptient_data.index).reset_index(drop=True)\n",
    "test_data = test_ptient_data.reset_index(drop=True)\n",
    "grid = GridSearchCV(LinearSVC(penalty='l1',\n",
    "                              loss='squared_hinge',\n",
    "                              dual=False,\n",
    "                              fit_intercept=True),\n",
    "                    param_grid={\n",
    "                        \"C\": [0.1, 0.5, 1, 5, 10],\n",
    "                        \"max_iter\": [1000, 5000, 10000, 50000, 100000]\n",
    "                    },\n",
    "                    cv=5)\n",
    "x_train, y_train = SampleGenerator(train_data, boundary, housekeeping)\n",
    "x_train = x_train[:, (repeat_mask ==100)]\n",
    "grid.fit(x_train, y_train)\n",
    "print(\"The best parameters are %s with a score of %0.2f\" %\n",
    "      (grid.best_params_, grid.best_score_))\n",
    "\n",
    "model = LinearSVC(penalty='l1',\n",
    "                  loss='squared_hinge',\n",
    "                  dual=False,\n",
    "                  fit_intercept=True,\n",
    "                  **grid.best_params_)\n",
    "model.fit(x_train, y_train)\n",
    "x_test, y_test = SampleGenerator(test_data, boundary, housekeeping)\n",
    "x_test = x_test[:, (repeat_mask == 100)]\n",
    "expected = y_test\n",
    "predicted = model.predict(x_test)\n",
    "\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, predicted)\n",
    "accuracy = metrics.accuracy_score(y_test, predicted)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08bca10ddddfa5ec2218c3ca88431b2016326f124aa4b8d7dee67ba697a17c3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
